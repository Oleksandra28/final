\cleardoublepage

%---------------------------------------------------------------------------------------------------------------------
\chapter{Introduction}
\label{introduction}

In this chapter, we first introduce the basic terminology in the field of Machine Learning that we use thoughout this document in Section \ref{basicterminology}, and then provide background on disaster management in Section \ref{backgrounddisaster}.

We state the main problem addressed in Section \ref{problemdefinition}, where we also give a high-level overview of the approaches used in this work. 

%---------------------------------------------------------------------------------------------------------------------
\section{Basic Terminology}
\label{basicterminology}

An agent is learning if it improves its performance on future tasks after making observations
about the world, as it is defined in ~\citep{rn}. The examples of tasks may include: 
\begin{itemize}
  \item identify a given email as spam or non-spam
  \item predict housing prices for a given location
  \item determine if there is a specific object in a given image
  \item categorize news articles into topics such as politics, sports, entertainment, etc
\end{itemize}

Machine Learning algorithms use feature based representations for instances, where each instance is represented using a collection of features $f_1,f_2,\cdots,f_n$ ~\citep{tom}. An instance is a single object of the world from which a model will be learned, or on which a model will be used (e.g., for prediction). In most machine learning work, instances are described by feature vectors; some work uses more complex representations (e.g., containing relations between instances or between parts of instances) ~\citep{terms}. For example, an instance of the task "identify a given email as spam or non-spam" may be a text of an email represented as bag-of-words ~\citep{tom}. In this work, we use the words "instance" and "example" interchangeably. 

Two major types of learning are distinguished: supervised and unsupervised learning.

In supervised learning the agent is given a training set of N examples, which could be seen as input-–output pairs $(x_1, y_1), (x_2, y_2), \cdots(x_N, y_N)$, where each $y_j$ was generated by an unknown function $y = f(x)$. The task is to should discover a function $h$ that approximates the true function $f$ ~\citep{rn}. Identifying a given email as spam or non-spam is an example of a supervised learning task since training a model requires labeled instances, i.e. emails marked as spam and non-spam. 

In unsupervised learning the agent learns patterns in the input even though no explicit feedback is supplied ~\citep{rn}; essentially, it means that only $(x_1), (x_2), \cdots(x_N)$ are provided. Categorizing news articles into topics such as politics, sports, entertainment, etc is an example of an unsupervised learning task since it requires finding similarity between different news articles and clustering them together, with no prior labels provided. 

A classifier, or a classification model, is defined as a mapping from unlabeled instances to (discrete) classes. Classifiers have a form (e.g., decision tree) plus an interpretation procedure (including how to handle unknowns, etc.). Some classifiers also provide probability estimates (scores), which can be thresholded to yield a discrete class decision thereby taking into account a utility function ~\citep{terms}. 

In this work, we primarily focus on the first type of learning in the attempt to take advantage of labeled data. We also explore unsupervised methods to make use of unlabeled data. 


%---------------------------------------------------------------------------------------------------------------------
\section{Background on Disaster Management}
\label{backgrounddisaster}

Social media have become an integral part of disaster response. Twitter is one of the social media networks that can fill the void in areas where cell phone service might be lost, and where people look to resources to keep informed, locate loved ones, notify authorities and express support ~\citep{scientificamer}.

For instance, the Federal Emergency Management Agency (FEMA) wrote in its 2013 National Preparedness report that during and immediately following Hurricane Sandy, “users sent more than 20 million Sandy-related Twitter posts, or “tweets,” despite the loss of cell phone service during the peak of the storm.” New Jersey’s largest utility company, PSE\&G, said at the subcommittee hearing that during Sandy they staffed up their Twitter feeds and used them to send word about the daily locations of their giant tents and generators ~\citep{scientificamer}.

Following the Boston Marathon bombings, one quarter of Americans reportedly looked to Facebook, Twitter and other social networking sites for information, according to The Pew Research Center. The sites also formed a key part of the information cycle: when the Boston Police Department posted its final "CAPTURED!!!" tweet of the manhunt, more than 140,000 people retweeted it ~\citep{scientificamer}.

Furthermore, the National Disaster Management Authority (NDMA) spearheads an integrated approach to disaster management for the Government of India. They use Twitter to receive reports of damage from the field, and crowdsource the data to get sophisticated insights into what is happening in a region. Often the scale of events is so big, plotting data from Twitter helps to prioritize areas most affected. Twitter also helps the government communicate to the people affected what relief is available to them and where they can go to receive it ~\citep{twittercrisisblog}.

%---------------------------------------------------------------------------------------------------------------------
\section{Problem Definition}
\label{problemdefinition}

According to the members of the National Disaster Management Authority (NDMA), some of the challenges of using social media during disasters include tweaking the strategy in real time as ‘the disaster you planned for is not the disaster that happens’, using the right hashtags as part of a ‘good Twitter strategy', and devising ways to deal with misinformation and rumours ~\citep{twittercrisisblog}.

Machine learning methods may serve as an efficient solution to identify relevant tweets about disasters ~\citep{tweedr}. In fact, supervised machine learning methods have been used extensively because of the availability of labeled training data --- tweets about previous disasters --- that have been labeled by crowdsourcing, and thus, can be used to train a classification model ~\citep{starbird}. Furthermore, since tweets are primarily texts, natural language processing (NLP) methods for disaster management have been also researched by ~\citep{sakaki} and ~\citep{terpstra}. 

However, there are still many challenges in using relevant data from Twitter to help disaster response teams save people’s lives and property ~\citep{mendoza}. One of the major challenges is that for a current on-going disaster, no labeled data is available. Obviously, labeling data is an expensive, time-consuming and error-prone process. Thus, using supervised learning methods to aid disaster response may not be time-efficient. Yet, labeled data for a previous disaster may be available. We call a previous disaster data \textit{source} and current on-going disaster data \textit{target}. In addition, even though labeled target data is not available, unlabeled target data may still be available, and we explore its usage throughout this work.

We define the task as follows: given labeled source tweets, train a model to classify target tweets as \textit{relevant/not relevant} i.e. \textit{about disaster/not about disaster}. Yet, given that the distributions of source and target data are generally different, our model may not perform well. 

One of the key challenges is adapting the classifier to perform well on a target unlabeled domain. The process of adapting a classifier to make predictions on an unseen domain where labeled data is unavailable is called Domain Adaptation. In supervised machine learning, the general assumption is that both training and testing data come from the same distribution. This holds true for data coming from the saame domain. However, in real-world classification problems training and testing data may come from different domains. As a result, the performance of a classifier may drop significantly. Thus, it becomes essential to adapt the classifier trained on one domain to give accurate prediction on another domain. 

We raise the following questions, which we attempt to answer throughout this work:

\begin{itemize}
  \item is labeled source data sufficient to train a supervised learning model to make accurate predictions on target data?
  \item does single-source domain adaptation result in the higher accuracy?
  \item does multi-source domain adaptation result in the higher accuracy as compared to single-source domain adaptation?
\end{itemize}

In our experiments, we repeatedly take 2012 Sandy Hurricane, 2013 Queensland Floods, 2013 Boston Bombings, 2013 West Texas Explosion, 2013 Oklahoma Tornado and 2013 Alberta Floods tweets and combine them in source-target pairs based on the chronological order of the actual events. Our motivation is as follows: a \textit{source} disaster happens earlier than a \textit{target} disaster and thus, training a classifier in such a way complies more with future real-life applications.