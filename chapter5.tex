\cleardoublepage

\chapter{Multi-source Domain Adaptation Approach}
\label{mdachapter}

In this chapter, we first define the problem of learning from multiple sources in Section ~\ref{mdaproblemdefinitions}, and then describe the multi-source domain adaptation algorithm ("MDA") proposed in the paper ~\citep{mda} in Section ~\ref{mdaalg}. Finally, we discuss the results obtained after applying MDA on our data in Section ~\ref{mdaexperiments}.


%---------------------------------------------------------------------------------------------------------------------
\section{Problem Definition}
\label{mdaproblemdefinitions}

We define our goal as follows: given tweets from several source domains, train a model to classify tweets from a target domain. The general intuition is that more data should improve performance. Yet, adding more source data, even when expressed via target features, may not necessarily contribute to a higher classification accuracy. In addition, labeled target data, again, is not available. Thus, we explore one of the methods presented in ~\citep{mda}, specificaly, we adopt the idea of modeling the target domain as a linear mixture of the source domains. 

%---------------------------------------------------------------------------------------------------------------------
\section{Multi-source Domain Adaptation Algorithm}
\label{mdaalg}

Notation: \\
$X$ -- features \\
$Y$ -- class labels \\
$P_Y$ -- a distribution of labels / cause \\
$P_{X|Y}$ -- a causal mechanism to generate effect $X$ from cause $Y$ \\
$V_S$ -- a domain-specific selection variable \\
$P_{X|Y, V_S}$ -- a conditional $P_{X|Y}$ in the domain associated with $V_S$

\citet{mda} focus on multi-source domain adaptation from a causal point of view, precisely, they focus on a typical domain adaptation scenario where both $P_Y$ and $P_{X|Y}$ change across domains, but their changes are independent from each other. They assume that the source domains contain rich information such that for each class, $P_{X|Y}^t$ can be approximated by a linear mixture of $P_{X|Y}$ on source domains. 

\citet{mda} discuss several possible domain adaptation situations and their solutions, and we research the case where they model the target as a linear mixture of the sources. 

Consider $P_{X|Y, V_S}$ as the mechanism to generate features from the class labels given the domain. Next, imagine that there exist $L$ elementary "sub-mechanisms", or class conditional feature distributions, $\tilde{P}_{X|Y}^{(l)}$, $l = 1, \cdots, L$, so that the mechanism in each domain, $P_{X|Y, V_S}$, is a mixture of those sub-mechanisms, i.e. $P_{X|Y=c_j, V_S} = \sum_{l=1}^{L} \tilde{a}_{V_S, j, l} \tilde{P}_{X|Y}^{(l)}$, where $\aa_{V_S, j, l}$ depend on both $V_S$ and $j, {\aa}_{V_S, l} \geqslant 0$, and $\sum_{l=1}^{L} \aa_{V_S, j, l} = 1$. 

Consequently, in the multi-source domain adaptation scenario, if for each $j$, the rank of $\left \{P_{X|Y=c_j}^{(i)} | i = 1, \cdots, n \right \}$ is equal to $L$, $P_{X|Y=c_j}^{t}$ can always be represented as a linear mixture of $P_{X|Y=c_j}^{(i)}$, as \citet{mda} state. For each $y$, $P_{X|Y=y}^{t}$ is a mixture of $P_{X|Y=y}$ on the source domains, i.e. there exist $a_{ij}$, which satisfy the constraint $\sum_{i=1}^{n} \aa_{ij} = 1$ for all $j$, such that \[ P_{X|Y=c_j}^{new} = \sum_{i=1}^{n} a_{ij}P_{X|Y=c_j}^{(i)}\] is equal to $P_{X|Y=c_j}^{t}$, where $c_j$ is the $j$th possible value of $Y$. \citet{mda} denote by $P_{Y}^{new}$ a marginal distribution of $Y$, and use $P_{Y}^{new}(c_j)$ as shorthand for $P_{Y}^{new}(Y=c_j)$. The corresponding joint distribution is $P_{X,Y=c_j}^{new}=P_{Y}^{new}(c_j)P_{X|Y=c_j}^{new}$, and the marginal distribution of $X$ is $P_{X}^{new}=\sum_{j=1}^{C}P_{Y}^{new}\sum_{i=1}^{n}a_{ij}P_{X|Y=c_j}^{(i)}$. \citet{mda} aim to match $P_{X}^{new}$ with $P_{X}^{t}$ by tuning the parameters $a_{ij}$ and $P_{Y}^{new}(c_j)$. The constraints are: $P_{Y}^{new}(c_j) \geqslant 0$, and $\sum_{j=1}^{C}P_{Y}^{new}(c_j)=1$. Let $\beta_{ij} \triangleq P_{Y}^{new}(c_{ij})a_{ij}$, which satisfy the condition $\sum_{j=1}^{C}\sum_{i=1}^{n}\beta_{ij} = 1$. When \citet{mda} find the values of $\beta_{ij}$, they reconstruct $p_{Y}^{new}$ and $a_{ij}$ by $P_{Y}^{new}(c_j) = \sum_{i=1}^{n}\beta_{ij}$, and $a_{ij} = \frac{\beta_{ij}}{P_{Y}^{new}(c_j)}$.

%---------------------------------------------------------------------------------------------------------------------
\section{Experiments and Results}
\label{mdaexperiments}

We present the results obtained when no domain adaptation is performed in Table \ref{multisourcenoda}. The source data is expressed via target features and is described in Table \ref{pairstablemulti}. The binary representation (i.e. \textit{0/1}) for both source and target data is used. The source data from different domains is merged together and treated as a whole.

The target data is divided into five folds for cross-validation ~\citep{hastie}. Each fold in turn is used for testing, and the accuracy is recorded in each run. The average results are reported. The number of instances per class in the source data is varied: the results are recorded for source data having $500$ instances per class, $1000$ instances per class, and $2000$ instances per class. All source data is also used as training data. 

As we can see, using more source data generally improves the performance of the classifier.

%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Pairs of Mutli-source--Target Disasters}
    \begin{tabular}[c]{|c|c|c|}
        \hline
        Pair & Source Disaster Event & Target Disaster Event  \\
        \hline
        $SH \rightarrow BB$ & 2012 Sandy Hurricane & 2013 Boston Bombings \\
        $QF \rightarrow BB$ & 2013 Queensland Floods & 2013 Boston Bombings \\

        $SH \rightarrow WT$ & 2012 Sandy Hurricane & 2013 West Texas Explosion \\
        $BB \rightarrow WT$ & 2013 Boston Bombings & 2013 West Texas Explosion \\

        $SH \rightarrow OT$ & 2012 Sandy Hurricane & 2013 Oklahoma Tornado \\
        $QF \rightarrow OT$ & 2013 Queensland Floods & 2013 Oklahoma Tornado \\
        $BB \rightarrow OT$ & 2013 Boston Bombings & 2013 Oklahoma Tornado  \\

        $SH \rightarrow AF$ & 2012 Sandy Hurricane & 2013 Alberta Floods \\
        $QF \rightarrow AF$ & 2013 Queensland Floods & 2013 Alberta Floods \\
        $BB \rightarrow AF$ & 2013 Boston Bombings & 2013 Alberta Floods \\
        \hline
    \end{tabular}
    \label{pairstablemulti}
   \end{center}
\end{table}
%==================Table end


%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Bernoulli Naive Bayes on multi-source data when no domain adaptation is performed}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline
                             
        $SH, QF \rightarrow BB$ &  0.615428571 & 0.735657143 & 0.596 & 0.688685714  \\ %
                            
        $SH, BB \rightarrow WT$ &  0.850383658	& 0.877821702 & 0.90255107 & 	0.930555488  \\%
                             
        $SH, QF, BB \rightarrow OT$ &  0.82550285	& 0.845725901 & 	0.84960058 & 	0.826957615\\%
                             
        $SH, QF, BB \rightarrow AF$ & 0.72804768 &	0.752038286 & 0.768482834	& 0.775057287 \\ %


        \hline
    \end{tabular}
    \label{multisourcenoda}
   \end{center}
\end{table}
%==================Table end

We first choose a pair of $SH, QF, BB \rightarrow AF$ to experiment with the idea proposed in \citep{mda}. The results are presented in Table \ref{multisourcemda1}. The columns $500$, $1000$, $2000$ and $Total$ mean that $500$, $1000$, $2000$ and all samples per class per source are taken, respectively, and weights are obtained for each of them separately. The weights are presented in Table \ref{sampleweights}, which can be interpreted as follows: the weights for a specific class ($0$ / "Neg" or $1$ / "Pos") across the sources should sum up to $1$. The more one source is similar to the target, the more weight it receives. Also, when the source data is balanced, i.e. $500$, $1000$, $2000$ instances per class per source are used, the weights do not differ much, for example, the weights for the negative class when $500$ intances are used, are almost identical across all the three sources: $0.33779$, $0.33601$ and $0.3262$ for $SH$, $QF$ and $BB$, respectively.


\begin{table}[ht]
    \begin{center}
    \caption{Weights obtained for $SH, QF, BB \rightarrow AF$ (sigma=1.1314)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}

        \hline
        Source & Class & 500 & 1000 & 2000 & Total \\
        \hline
    	\multirow{2}{*}{$SH$} &Neg & 0.33779 & 0.33864 & 0.35149 & 0.38421 \\ & Pos  & 0.34725 & 0.36871 & 0.37865  & 0.38766 \\ 
    	\hline
    	\multirow{2}{*}{$QF$} &Neg  & 0.33601 & 0.34244 & 0.34336 & 0.38544 \\ & Pos  & 0.30849 & 0.25878 & 0.23889  & 0.15128 \\
    	\hline
    	\multirow{2}{*}{$BB$} &Neg  & 0.3262  & 0.31891 & 0.30515 & 0.23035 \\ & Pos  & 0.34426 & 0.37252 & 0.38246  & 0.46105 \\ 

        \hline
    \end{tabular}
    \label{sampleweights}
   \end{center}
\end{table}



The initial value of $sigma$ equal to $1.1314$ is used in the original paper, so we use it as well. However, since the accuracy decreases, we also experiment with different values for $sigma$. The results are presented in Tables \ref{multisourcesigma00001}, \ref{multisourcesigma001}, \ref{multisourcesigma1}, \ref{multisourcesigma10}  for values of sigma set to $0.0001, 0.01, 1, 10, 100$ respectively.

\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Gaussian Naive Bayes on multi-source data after applying MDA (sigma=1.1314)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline
                             
        $SH, QF, BB \rightarrow AF$ & 0.705416920268 &  0.753499695679 & 0.723067559343 & 0.709068776628 \\ %

        \hline
    \end{tabular}
    \label{multisourcemda1}
   \end{center}
\end{table}


%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Gaussian Naive Bayes on multi-source data after applying MDA (sigma=0.0001)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline                             
        $SH, QF, BB \rightarrow AF$ & 0.703590992 & 0.723676202  & 0.723676202   & 0.69872185 \\ %
        \hline
    \end{tabular}
    \label{multisourcesigma00001}
   \end{center}
\end{table}
%==================Table end




%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Gaussian Naive Bayes on multi-source data after applying MDA (sigma=0.01)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline                             
        $SH, QF, BB \rightarrow AF$ & 0.703590992 & 0.723676202 & 0.723676202 & 0.69872185 \\ %
        \hline
    \end{tabular}
    \label{multisourcesigma001}
   \end{center}
\end{table}
%==================Table end



%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Gaussian Naive Bayes on multi-source data after applying MDA (sigma=1)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline                             
        $SH, QF, BB \rightarrow AF$ & 0.703590992 & 0.723676202 & 0.723676202 & 0.69872185 \\ %
        \hline
    \end{tabular}
    \label{multisourcesigma1}
   \end{center}
\end{table}
%==================Table end



%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Gaussian Naive Bayes on multi-source data after applying MDA (sigma=10)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline                             
        $SH, QF, BB \rightarrow AF$ & 0.702982349 & 0.721241631 & 0.717589775 & 0.687157638 \\ %
        \hline
    \end{tabular}
    \label{multisourcesigma10}
   \end{center}
\end{table}
%==================Table end



%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Gaussian Naive Bayes on multi-source data after applying MDA (sigma=100)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline                             
        $SH, QF, BB \rightarrow AF$ & 0.704808278 & 0.693852708 & 0.661594644 & 0.637857578 \\ %
        \hline
    \end{tabular}
    \label{multisourcesigma100}
   \end{center}
\end{table}
%==================Table end


We decide to further experiment with multi-source domain adaptation. Precisely, motivated by the improved results discussed in Chapter \ref{coralchapter}, we apply the same logic to the multi-source case: we first select features based on Variance Threshold, and then run CORAL. The transformed data is used in training of the Gaussian Naive Bayes classifier. The results are presented in Table \ref{multisourcenvt99coral}. We can see that the accuracy increases across all pairs, e.g. from $0.61$, when no domain adaptation is performed, to $0.81$, after applying feature selection and CORAL, for the pair $SH, QF \rightarrow BB$, when $500$ samples per class are used.


%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Gaussian Naive Bayes on multi-source data after applying Variance Threshold (0.99) and CORAL}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline
                             
        $SH, QF \rightarrow BB$ & 0.811885714 & 	0.7952	& 0.783542857	& 0.795771429  \\ %
                            
        $SH, BB \rightarrow WT$ & 0.924119594 & 	0.943992949	& 0.94241173 & 	0.932474734   \\%
                             
        $SH, QF, BB \rightarrow OT$ &  0.868976026	& 0.862679013	& 0.820055275	& 0.840034068 \\%
                             
        $SH, QF, BB \rightarrow AF$ & 0.862623332	& 0.866762695	& 0.879185605	& 0.857628755 \\ %


        \hline
    \end{tabular}
    \label{multisourcenvt99coral}
   \end{center}
\end{table}
%==================Table end

In addition, we also confirm that applying CORAL contributes to the accuracy improvement by running an additional set of experiments. Precisely, we do not apply CORAL but we do apply feature selection (Variance Threshold). The results are presented in Table \ref{multisourcenvt99}. We can see that feature selection by itself improves performance, although the accuracy is still lower than when CORAL is applied.


%==================Table begin
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy with Bernoulli Naive Bayes on multi-source data after applying Variance Threshold (0.99)}
    \begin{tabular}[c]{|c|c|c|c|c|c|}
        \hline
        Pair & 500 & 1000 & 2000 & Total \\
        \hline
                             
        $SH, QF \rightarrow BB$ & 0.727428571 &  0.7704  & 0.734285714 & 0.749714286 \\ %
                            
        $SH, BB \rightarrow WT$ &  0.92118289   &  0.928410509  & 0.926264574  & 0.924796731  {}\\%
                             
        $SH, QF, BB \rightarrow OT$ &  0.836763617  &  0.823445032  & 0.82925727   & 0.839428228 \\%
                             
        $SH, QF, BB \rightarrow AF$ & 0.740836072    & 0.741079455  & 0.748629961  & 0.73413774 \\ %


        \hline
    \end{tabular}
    \label{multisourcenvt99}
   \end{center}
\end{table}
%==================Table end