\cleardoublepage

%---------------------------------------------------------------------------------------------------------------------
\chapter{Introduction}
\label{introduction}

In this chapter, we first introduce the basic terminology in the field of Machine Learning that we use thoughout this document in Section \ref{basicterminology}, and then state the main problem addressed in Section \ref{problemdefinition}. 

We discuss motivation for domain adaptation in Section \ref{motivation}, where we also give a high-level overview of the approaches used in this work. 

Additionally,  in Section \ref{dataset} we describe the data used in the experiments, as well as the operations performed on the data (e.g. cleaning). 

We also discuss the preliminary results in Section \ref{preresults}, which are obtained on the data when no domain adaptation is performed.

%---------------------------------------------------------------------------------------------------------------------
\section{Basic Terminology}
\label{basicterminology}

An agent is learning if it improves its performance on future tasks after making observations
about the world, as it is defined in ~\citep{rn}. The examples of tasks may include: 
\begin{itemize}
  \item identify a given email as spam or non-spam
  \item predict housing prices for a given location
  \item determine if there is a specific object in a given image
  \item categorize news articles into topics such as politics, sports, entertainment, etc
\end{itemize}

Machine Learning algorithms use feature based representations for instances, where each instance is represented using a collection of features $f_1,f_2,\cdots,f_n$ ~\citep{tom}. An instance is a single object of the world from which a model will be learned, or on which a model will be used (e.g., for prediction). In most machine learning work, instances are described by feature vectors; some work uses more complex representations (e.g., containing relations between instances or between parts of instances) ~\citep{terms}. For example, an instance of the task "identify a given email as spam or non-spam" may be a text of an email represented as bag-of-words ~\citep{bagofwords}. In this work, we use the words "instance" and "example" interchangeably. 

Two major types of learning are distinguished: supervised and unsupervised learning.

In supervised learning the agent is given a training set of N examples, which could be seen as input-–output pairs $(x_1, y_1), (x_2, y_2), \cdots(x_N, y_N)$, where each $y_j$ was generated by an unknown function $y = f(x)$. The task is to should discover a function $h$ that approximates the true function $f$ ~\citep{rn}. Identifying a given email as spam or non-spam is an example of a supervised learning task since training a model requires labeled instances, i.e. emails marked as spam and non-spam. 

In unsupervised learning the agent learns patterns in the input even though no explicit feedback is supplied ~\citep{rn}; essentially, it means that only $(x_1), (x_2), \cdots(x_N)$ are provided. Categorizing news articles into topics such as politics, sports, entertainment, etc is an example of an unsupervised learning task since it requires finding similarity between different news articles and clustering them together, with no prior labels provided. 

A classifier, or a classification model, is defined as a mapping from unlabeled instances to (discrete) classes. Classifiers have a form (e.g., decision tree) plus an interpretation procedure (including how to handle unknowns, etc.). Some classifiers also provide probability estimates (scores), which can be thresholded to yield a discrete class decision thereby taking into account a utility function ~\citep{terms}. 

In this work, we primarily focus on the first type of learning in the attempt to take advantage of labeled data. We also explore unsupervised methods to make use of unlabeled data. 


%---------------------------------------------------------------------------------------------------------------------
\section{Background on Disaster Management}
\label{backgrounddisaster}

Social media have become an integral part of disaster response. Twitter is one of the social media networks that can fill the void in areas where cell phone service might be lost, and where people look to resources to keep informed, locate loved ones, notify authorities and express support ~\citep{scientificamer}.

For instance, the Federal Emergency Management Agency (FEMA) wrote in its 2013 National Preparedness report that during and immediately following Hurricane Sandy, “users sent more than 20 million Sandy-related Twitter posts, or “tweets,” despite the loss of cell phone service during the peak of the storm.” New Jersey’s largest utility company, PSE\&G, said at the subcommittee hearing that during Sandy they staffed up their Twitter feeds and used them to send word about the daily locations of their giant tents and generators ~\citep{scientificamer}.

Following the Boston Marathon bombings, one quarter of Americans reportedly looked to Facebook, Twitter and other social networking sites for information, according to The Pew Research Center. The sites also formed a key part of the information cycle: when the Boston Police Department posted its final “CAPTURED!!!” tweet of the manhunt, more than 140,000 people retweeted it ~\citep{scientificamer}.

Furthermore, the National Disaster Management Authority (NDMA) spearheads an integrated approach to disaster management for the Government of India. They use Twitter to receive reports of damage from the field, and crowdsource the data to get sophisticated insights into what is happening in a region. Often the scale of events is so big, plotting data from Twitter helps to prioritize areas most affected. Twitter also helps the government communicate to the people affected what relief is available to them and where they can go to receive it ~\citep{twittercrisisblog}.

According to the members of NDMA, some of the challenges of using social media during disasters include tweaking the strategy in real time as ‘the disaster you planned for is not the disaster that happens’, using the right hashtags as part of a ‘good Twitter strategy', and devising ways to deal with misinformation and rumours ~\citep{twittercrisisblog}.

%---------------------------------------------------------------------------------------------------------------------
\section{Problem Definition}
\label{problemdefinition}

Machine learning methods may serve as an efficient solution to identify relevant tweets about disasters ~\citep{tweedr}. In fact, supervised machine learning methods have been used extensively because of the availability of labeled training data --- tweets about previous disasters --- that have been labeled by crowdsourcing, and thus, can be used to train a classification model ~\citep{starbird}. Furthermore, since tweets are primarily texts, natural language processing (NLP) methods for disaster management have been also researched ~\citep{sakaki}, ~\citep{terpstra}. 
However, for a current on-going disaster, no labeled data is available. Obviously, labeling data is an expensive, time-consuming and error-prone process. Thus, using supervised learning methods to aid disaster response may not be time-efficient. Yet, labeled data for a previous disaster may be available. We call a previous disaster data \textit{source} and current on-going disaster data \textit{target}. Moreover, despite the great progress in 

We define the task as follows: given source tweets only, train a model to classify target tweets as \textit{relevant/not relevant} i.e. \textit{about disaster/not about disaster}. Yet, given that the distributions of source and target data are generally different, our model may not perform well. In addition, labeled target data is not available, nevertheless, unlabeled target data may be available.

One of the key challenges is adapting the classifier to perform well on a target unlabeled domain. The process of adapting a classifier to make predictions on an unseen domain where labeled data is unavailable is called Domain Adaptation. In supervised machine learning, the general assumption is that both training and testing data come from the same distribution. However, in real-world classification problems this assumption rarely holds true when training and testing data come form different domains. As a result, the performance of a classifier may drop significantly. Thus, it becomes essential to adapt the classifier trained on one domain to give accurate prediction on another domain.

We raise the following questions, which we attempt to answer throughout this work:

\begin{itemize}
  \item is labeled source data sufficient to train a supervised learning model to make accurate predictions on target data?
  \item does single-source domain adaptation result in the higher accuracy?
  \item doed multi-source domain adaptation result in the higher accuracy as compared to single-source domain adaptation?
\end{itemize}

In our experiments, we repeatedly take 2012 Sandy Hurricane, 2013 Queensland Floods, 2013 Boston Bombings, 2013 West Texas Explosion, 2013 Oklahoma Tornado and 2013 Alberta Floods tweets and combine them in source-target pairs based on the chronological order of the actual events. Our motivation is as follows: a \textit{source} disaster happens earlier than a \textit{target} disaster and thus, training a classifier in such a way complies more with future real-life applications.


%---------------------------------------------------------------------------------------------------------------------
\section{Data Description}
\label{dataset}

In this work, we use the dataset CrisisLexT6 ~\citep{data}. The dataset consists of ~60K tweets\footnote{A tweet is a short text (up to 140 characters) that users of Twitter can post on Twitter.com} posted during 6 crisis events in 2012 and 2013.  ~60,000 tweets (10,000 in each collection) were labeled by crowdsourcing workers according to relatedness (as \textit{on-topic} or \textit{off-topic}). The \textit{on-topic} tweets are labeled as \textit{1}, and the \textit{off-topic} tweets are labeled as \textit{0}. 

We select 2013 Alberta Floods tweets as \textit{target} and 2012 Sandy Hurricane, 2013 Boston Bombings, 2013 Queensland Floods tweets as \textit{source}, which are described in detail in Table ~\ref{table1} and in Table ~\ref{table2}. We also run experiments on the additional pairs of source and target described in Table ~\ref{tableadddata} in Subsection ~\ref{varthressub}.

The tweets are preprocessed before they are used in training, domain adaptation and testing stages.
The following cleaning steps have been taken ~\citep{twitterda}:

\begin{itemize}
  \item non-printable, ASCII characters are removed, as they are generally regarded as noise rather than useful information.
  \item printable HTML entities are converted into their corresponding ASCII equivalents
  \item URLs, email addresses, and usernames are replaced with a URL/email/username placeholder for each type of entity, respectively, under the assumption that those features could be predictive 
  \item numbers, punctuation signs and hashtags are kept under the assumption that numbers could be indicative of an address, while punctuation/emoticons and hashtags could be indicative of emotions 
  \item RT (i.e., retweet) are removed under the assumptions that such features are not informative for our classification tasks
  \item duplicate tweets and empty tweets (that have no characters left after the cleaning) are removed from the data sets
\end{itemize}

After preprocessing, the source tweets are expressed via target features i.e. via words that occur in the target tweets. The bag-of-words ~\citep{bagofwords} representation is used to represent tweets as vectors of features. 

After running the preliminary experiments discussed in Section \ref{preresults} both with \textit{0/1} representation and \textit{counts} representation using Naive Bayes Classifier ~\citep{nb}, the results obtained in Table \ref{table3} have shown that \textit{0/1} representation gives better performance as compared to the \textit{counts} representation results presented in Table \ref{table4}. Consequently, in the further experiments the \textit{0/1} representation is used.


%==================Table 1 begin
\begin{table}[ht]
    \begin{center}
    \caption{Target and Source datasets}
    \begin{tabular}[c]{|c|c|c|c|}
        \hline
        Target & Source 1 & Source 2 & Source 3 \\
        \hline
        2013 Alberta Floods & 2012 Sandy Hurricane & 2013 Queensland Floods & 2013 Boston Bombings \\
        \hline
    \end{tabular}
    \label{table1}
   \end{center}
\end{table}
%==================Table 1 end

%==================Table 2 begin
\begin{table}[ht]
    \begin{center}
    \caption{Selected disaster events from CrisisLexT6 dataset}
    \begin{tabular}[c]{|c|c|c|c|}
        \hline
        Disaster Event & On-topic & Off-topic & Total \\
        \hline
        2013 Alberta Floods & 3497 & 4714 & 8211 \\
        2012 Sandy Hurricane & 5261 & 3752 & 9013 \\
        2013 Queensland Floods & 3236 & 4550 & 7786\\
        2013 Boston Bombings & 4441 & 4309 & 8211 \\
        \hline
    \end{tabular}
    \label{table2}
   \end{center}
\end{table}
%==================Table 2 end

%---------------------------------------------------------------------------------------------------------------------
\section{Preliminary results without domain adaptation}
\label{preresults}

We present the results obtained when no domain adaptation is performed. The source data is expressed via target features. Then, two representations are tested: \textit{0/1} and \textit{counts} to determine which representation is more promising i.e. gives better results; Bernoulli Naive Bayes and Multinomial Naive Nayes classifiers are used to classify target data respectively. 

The target data is divided into five folds for cross-validation ~\citep{hastie}. Each fold in turn is used for testing, and the accuracy is recorded in each run. The average results are reported. The number of instances per class in the source data is varied: the results are recorded for source data having $500$ instances per class, $1000$ instances per class, and $2000$ instances per class. All source data is also used as training data, resulting in slightly better accuracy for $Source 2$ and $Source  3$ ($0.7888$ and $0.7421$ respectively as compared to $0.7810$ and $0.7346$ respectively for the number of instances equal to $2000$) in Table \ref{table3}. Generally, balanced training data (i.e. data that has equal number of instances per class) gives better performance.

The results in Table \ref{table3} and Table \ref{table4} indicate that \textit{0/1} representation is more efficient. One of the possible explanations might be that it is unlikely that words are repeatedly used in a single tweet since tweets are relatively short. Thus, keeping the actual counts does not provide additional relevant information.


%==================Table 3
\begin{table}[ht]
    \begin{center}

% +--------------------------------------------------------------------+
% | The table is created with this command
% |
% | \begin{tabular}[pos]{table spec}
% |
% | The "pos" argument specifies the vertical position of the table
% | relative to the baseline of the surrounding text.  Use t, b, or c
% | to specify alignment at the top, bottom, or center.
% |
% | The "table spec" command defines the format of the table
% |   l for a column of left-aligned text
% |   r for a column of right-aligned text
% |   c for centered text
% |   p{width} for a column containing justified text with line breaks
% |   | for a vertical line
% |
% |  In this example, the caption is made to appear above the table
% |  by positioning the \caption command before the \begin{tabular
% |  command. To position the caption below the table, insert the
% |  \caption command after the \end{tabular} command.
% +--------------------------------------------------------------------+

    \caption{Accuracy after running Bernoulli Naive Bayes. 0/1 representation of features}
    \begin{tabular}[c]{|c|c|c|c|c|}
        \hline
        Sources & 500 & 1000 & 2000 & All \\
        \hline
        Source 1 & 0.699183336 & 0.737669795 & 0.748874085 & 0.714163926 \\
        Source 2 & 0.759834769 & 0.764704356 & 0.781025693 & 0.788819876 \\
        Source 3 & 0.710511727 & 0.716357069 & 0.734625396 & 0.742175131 \\
        \hline
    \end{tabular}

    \label{table3}
   \end{center}
\end{table}
%==================Table 3 end


%==================Table 4 end
\begin{table}[ht]
    \begin{center}
    \caption{Accuracy after running Multinomial Naive Bayes. Counts representation of features}
    \begin{tabular}[c]{|c|c|c|c|c|}
        \hline
        Sources & 500 & 1000 & 2000 & All \\
        \hline
        Source 1 & 0.667883087 & 0.708560882 & 0.727195729 & 0.694068932 \\
        Source 2 & 0.749968752 & 0.758614519 & 0.772012294 & 0.786384119 \\
        Source 3 & 0.676287695 & 0.697966125 & 0.715016795 & 0.720984046 \\
        \hline
    \end{tabular}

    \label{table4}
   \end{center}
\end{table}
%==================Table 4 end


